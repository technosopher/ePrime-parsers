####################
# filtering-parser.py
####################
# This script parses one or more ePrime datafiles generated by Ophir et al.'s filterting task, and assumes that these output files are formatted as comma separated value files, with commas as field deliminators.  The script checks for and removes outliers in the data, and then calculates mean accuracy rates and response times (for the accurate trials) for each of the target-distractor combinations that Ophir et. al found to be useful for gauging multitasking ability (2_0, 2_2, 2_4, and 2_6).  The script outputs the resulting data in comma separated value format, with one line per input datafile.  
####################

import codecs
import numpy
import argparse

### Read datafile location information from the command line
filelist = []
parser = argparse.ArgumentParser()
parser.add_argument("-d", "--datafiles", type=str, nargs='+', help="List of one or more datafiles")
parser.add_argument("-f", "--filelist", type=str, help="Single file containing a list of datafiles")
parser.add_argument("-o", "--outfile", type=str, help="Output file name; default is filtering-report.csv")
args = parser.parse_args()
if args.filelist:
        f = open(args.filelist)
        for filename in f:
                filelist.append(filename.strip())
elif args.datafiles:
        filelist = args.datafiles
else:
        print "Specify the location of the datafile(s) using either the --datafiles or --filelist parameter.  Exiting..."
        exit()
if args.outfile:
        outfile = args.outfile
else:
        outfile = 'filtering-report.csv'

### Function declaration: id_outliers(list_of_RTs, check_depth, max_step)
# Takes a list of response times, an integer indicating how many RTs on either end of the sorted list should be evaluated, and an integer indicating how large of a jump between two sequential RTs in these end regions designates the start of the outliers. 
def id_outliers(list_of_RTs, check_depth, max_step):  
        if check_depth < 2:
                check_depth = 2
                if check_depth > len(list_of_RTs)/2:
                        return []
        list_of_RTs.sort()
        replace_RTs = []
        trigger = 0
        for i in range(len(list_of_RTs[:check_depth])-1,-1,-1):
                if trigger > 0:
                        replace_RTs.append([list_of_RTs[:check_depth][i],trigger])
                elif i < 0:
                        break
                elif list_of_RTs[:check_depth][i]-list_of_RTs[:check_depth][i-1] > max_step:
                        trigger = list_of_RTs[:check_depth][i]
        trigger = 0
        for i in range(len(list_of_RTs[-check_depth:])):
                if trigger > 0:
                        replace_RTs.append([list_of_RTs[-check_depth:][i],trigger])
                elif i == len(list_of_RTs[-check_depth:])-1:
                        break
                elif list_of_RTs[-check_depth:][i+1]-list_of_RTs[-check_depth:][i] > max_step:
                        trigger = list_of_RTs[-check_depth:][i]
#	DEBUG:
#       print list_of_RTs[:check_depth]
#       print list_of_RTs[-check_depth:]
#       print replace_RTs
        return replace_RTs #Returns a list of pairs of [Original RTs, Replacement RTs]

### Function declaration: extract_and_clean_data(file_name, trial_type_dictionary)
# Takes the name of a datafile as a string, and a pre-initialized dictionary into which the function sorts all trials by type (keys are the four possible target-distractor combinations, and values are empty lists).  The function returns a 3-item list of information about the file as a whole, including filename, subject ID, and overall accuracy rate.
def extract_and_clean_data(file_name, trial_type_dictionary):
	datarows = []
	rts_acc = []
	rts_inacc = []

	datafile = codecs.open(file_name.strip(),encoding='utf16')
	for row in datafile:
		if row.count(',') < 50:
			continue
		activerow = row.strip().split(',')
		if activerow[41].find("2_") > -1:
			if activerow[52] == '1' or activerow[52] == '0':
				if activerow[52] == '1':
					rts_acc.append(int(activerow[59]))
				else:
					rts_inacc.append(int(activerow[59]))
	
				activerow = activerow[1]+','+activerow[41]+','+activerow[52]+','+activerow[53]+','+activerow[59] 
						#Filename, Subject ID, 	Accuracy of response, Correct answer, Response time
				datarows.append(activerow)	
	datafile.close()

	replacerts = id_outliers(rts_acc, 6, 100)

	for row in datarows:
		activerow = row.strip().split(',')
		for rt in replacerts:
			if int(activerow[4]) == rt[0] and activerow[2] == '1':  
				activerow[4] = unicode(str(rt[1]))
		trial_type_dictionary[activerow[1][:3]].append(activerow)
		activerow = ','.join(activerow)

	return [file_name[file_name.rfind('/')+1:].strip(), datarows[0].strip().split(',')[0], str(float(len(rts_acc))/float(len(datarows)))]

### Main script routine begins here
summary = [['Filename', 'Subject', 'Acc_total', 'RT_2_0', 'Acc_2_0', 'RT_2_2', 'Acc_2_2', 'RT_2_4', 'Acc_2_4', 'RT_2_6', 'Acc_2_6']]
for filename in filelist:
	print "Now processing " + filename.strip()
	databytrialtype = {'2_0':[], '2_2':[], '2_4':[], '2_6':[]}
	filereport = extract_and_clean_data(filename, databytrialtype)

	for trialtype, dataarray in databytrialtype.iteritems():
		allrts = [int(datapoint[4]) for datapoint in dataarray]
		correctrts = [int(datapoint[4]) for datapoint in dataarray if datapoint[2] == '1']
		wrongrts = [int(datapoint[4]) for datapoint in dataarray if datapoint[2] == '0']
		filereport.append(str(numpy.mean(correctrts)))
		filereport.append(str(float(len(correctrts))/float(len(allrts))))

#		hits = [int(datapoint[4]) for datapoint in dataarray if datapoint[2] == '1' and datapoint[3] == '4']
#		falsealerts = [int(datapoint[4]) for datapoint in dataarray if datapoint[2] == '0' and datapoint == '4']
#		print "Mean for " + trialtype + " is: " + str(numpy.mean(correctrts))

	summary.append(filereport)
report = codecs.open(outfile,'w',encoding='utf16')
for row in summary:
	report.write(','.join(row) + "\n")
report.write("\n")
report.close();
